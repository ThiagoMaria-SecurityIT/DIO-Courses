# Projeto: Classifica√ß√£o de Imagens com Transfer Learning

### **Contexto do Projeto**

Este projeto foi desenvolvido como parte da jornada de aprendizado em Deep Learning, aplicando os conceitos estudados no bootcamp da DIO. O objetivo √© demonstrar a aplica√ß√£o pr√°tica da t√©cnica de **Transfer Learning** para resolver um problema de classifica√ß√£o de imagens.  
A tarefa consiste em construir um modelo de alta performance capaz de distinguir entre imagens de gatos e c√£es, utilizando a biblioteca TensorFlow/Keras.  
Este `README.md` serve como a documenta√ß√£o t√©cnica do projeto. Ele detalha o fluxo de trabalho completo, desde a configura√ß√£o do ambiente e a prepara√ß√£o do dataset `cats_vs_dogs`, at√© a constru√ß√£o, o treinamento e a avalia√ß√£o final do modelo.  

Cada se√ß√£o descreve uma etapa do processo, explicando as decis√µes tomadas e apresentando os blocos de c√≥digo correspondentes.  



## üìã √çndice
1.  [Introdu√ß√£o ao Transfer Learning](#1-introdu√ß√£o-ao-transfer-learning)
2.  [Configura√ß√£o do Ambiente no Colab](#2-configura√ß√£o-do-ambiente-no-colab)
3.  [Carregamento e Prepara√ß√£o do Dataset](#3-carregamento-e-prepara√ß√£o-do-dataset)
4.  [Pr√©-processamento e Aumento de Dados](#4-pr√©-processamento-e-aumento-de-dados)
5.  [Constru√ß√£o do Modelo com Transfer Learning](#5-constru√ß√£o-do-modelo-com-transfer-learning)
6.  [Treinamento em Duas Fases](#6-treinamento-em-duas-fases)
7.  [Avalia√ß√£o do Modelo](#7-avalia√ß√£o-do-modelo)
8.  [Conclus√£o e Pr√≥ximos Passos](#8-conclus√£o-e-pr√≥ximos-passos)

---

## 1. Introdu√ß√£o ao Transfer Learning

O **Transfer Learning (TL)** √© uma t√©cnica de Machine Learning que consiste em reutilizar um modelo pr√©-treinado em uma tarefa de grande escala (como a classifica√ß√£o de milh√µes de imagens do ImageNet) e adapt√°-lo para um novo problema. Isso permite alcan√ßar alta performance com menos dados e menor tempo de treinamento.

A ideia √© que as camadas iniciais de uma rede neural convolucional (CNN) aprendem a detectar caracter√≠sticas gen√©ricas (bordas, texturas, cores), que s√£o √∫teis para diversas tarefas de vis√£o computacional. N√≥s aproveitamos esses aprendizados e treinamos apenas as camadas finais para a nossa tarefa espec√≠fica (gatos vs. c√£es).

---

## 2. Configura√ß√£o do Ambiente no Colab

Primeiro, importamos as bibliotecas necess√°rias e verificamos a disponibilidade de uma GPU, o que acelera drasticamente o treinamento.

```python
# Importa√ß√µes principais
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model
from tensorflow.keras.applications import EfficientNetB0
import tensorflow_datasets as tfds

import numpy as np
import matplotlib.pyplot as plt

# Verifica a disponibilidade da GPU
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

# Configura√ß√µes para garantir resultados reproduz√≠veis
SEED = 42
tf.random.set_seed(SEED)
np.random.seed(SEED)
```

---

## 3. Carregamento e Prepara√ß√£o do Dataset

Utilizamos o dataset `cats_vs_dogs` dispon√≠vel no TensorFlow Datasets. Ele √© automaticamente baixado e dividido em conjuntos de treino (80%), valida√ß√£o (10%) e teste (10%).

```python
# Carrega o dataset com as divis√µes
(ds_train, ds_validation, ds_test), ds_info = tfds.load(
    'cats_vs_dogs',
    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],
    shuffle_files=True,
    as_supervised=True, # Retorna pares (imagem, label)
    with_info=True
)

# Exibe informa√ß√µes sobre o dataset
print(f"N√∫mero de classes: {ds_info.features['label'].num_classes}")
print(f"Nomes das classes: {ds_info.features['label'].names}")
print(f"Imagens de treino: {tf.data.experimental.cardinality(ds_train).numpy()}")
print(f"Imagens de valida√ß√£o: {tf.data.experimental.cardinality(ds_validation).numpy()}")
print(f"Imagens de teste: {tf.data.experimental.cardinality(ds_test).numpy()}")
```
**Sa√≠da esperada:**
```
N√∫mero de classes: 2
Nomes das classes: ['cat', 'dog']
Imagens de treino: 18610
Imagens de valida√ß√£o: 2326
Imagens de teste: 2326
```

---

## 4. Pr√©-processamento e Aumento de Dados

Para que as imagens possam ser usadas pelo modelo, elas precisam ser pr√©-processadas. Isso inclui:
*   **Redimensionamento:** Ajustar todas as imagens para o mesmo tamanho (`224x224` pixels).
*   **Normaliza√ß√£o:** Converter os valores dos pixels de `[0, 255]` para `[0, 1]`.
*   **Aumento de Dados (Data Augmentation):** Aplicar transforma√ß√µes aleat√≥rias (giros, zoom, etc.) apenas nas imagens de treino para que o modelo generalize melhor e evite overfitting.

```python
# Par√¢metros
IMG_SIZE = (224, 224) # Tamanho de entrada para EfficientNetB0
BATCH_SIZE = 32
AUTOTUNE = tf.data.AUTOTUNE

# Camada para aumento de dados
data_augmentation = keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
])

# Fun√ß√£o para pr√©-processar as imagens
def preprocess_image(image, label):
    image = tf.image.resize(image, IMG_SIZE)
    image = tf.cast(image, tf.float32) / 255.0 # Normaliza para [0, 1]
    return image, label

# Fun√ß√£o para preparar os datasets (aplicando pr√©-processamento e augmentation)
def prepare_dataset(dataset, is_training=False):
    dataset = dataset.map(preprocess_image, num_parallel_calls=AUTOTUNE)
    if is_training:
        dataset = dataset.shuffle(buffer_size=1000)
        dataset = dataset.map(lambda x, y: (data_augmentation(x), y), num_parallel_calls=AUTOTUNE)
    dataset = dataset.batch(BATCH_SIZE)
    dataset = dataset.prefetch(buffer_size=AUTOTUNE)
    return dataset

# Prepara os tr√™s conjuntos de dados
train_ds = prepare_dataset(ds_train, is_training=True)
val_ds = prepare_dataset(ds_validation)
test_ds = prepare_dataset(ds_test)
```

---

## 5. Constru√ß√£o do Modelo com Transfer Learning

Constru√≠mos nosso modelo usando a arquitetura **EfficientNetB0** como base.
1.  Carregamos o modelo pr√©-treinado no `ImageNet`, sem incluir sua camada classificadora final (`include_top=False`).
2.  **Congelamos** os pesos da base para que eles n√£o sejam alterados na primeira fase de treino.
3.  Adicionamos um novo classificador no topo, com uma camada de sa√≠da `Dense` de 1 neur√¥nio e ativa√ß√£o `sigmoid`, ideal para problemas de classifica√ß√£o bin√°ria.

```python
def create_model(input_shape=(224, 224, 3)):
    # Carrega o modelo base pr√©-treinado no ImageNet
    base_model = EfficientNetB0(
        include_top=False,
        weights='imagenet',
        input_shape=input_shape,
        pooling='avg'
    )

    # Congela a base para n√£o ser treinada inicialmente
    base_model.trainable = False

    # Constr√≥i o modelo final
    inputs = keras.Input(shape=input_shape)
    x = base_model(inputs, training=False) # Roda a base em modo de infer√™ncia
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dropout(0.3)(x)
    outputs = layers.Dense(1, activation='sigmoid')(x) # Sa√≠da para classifica√ß√£o bin√°ria

    model = Model(inputs, outputs)

    # Compila o modelo
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=1e-3),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    return model

# Cria o modelo
model = create_model()
model.summary()
```
*Note que a maioria dos par√¢metros √© "Non-trainable", o que √© esperado na fase de extra√ß√£o de caracter√≠sticas.*

---

## 6. Treinamento em Duas Fases

O treinamento √© dividido em duas etapas para obter os melhores resultados:

1.  **Feature Extraction:** Treinamos apenas o classificador que adicionamos. A base pr√©-treinada permanece congelada e atua como um extrator de caracter√≠sticas.
2.  **Fine-Tuning:** Ap√≥s a primeira fase, descongelamos as √∫ltimas camadas da base e continuamos o treinamento com uma taxa de aprendizado (`learning_rate`) muito baixa. Isso permite que o modelo ajuste fino as caracter√≠sticas aprendidas para o nosso dataset espec√≠fico.

```python
# --- Fase 1: Treinamento do Classificador ---
callbacks = [
    keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True),
    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3)
]

history_feature_extraction = model.fit(
    train_ds,
    epochs=30,
    validation_data=val_ds,
    callbacks=callbacks
)

# --- Fase 2: Fine-Tuning ---
base_model = model.layers[1]
base_model.trainable = True

# Descongela apenas as √∫ltimas 30 camadas
for layer in base_model.layers[:-30]:
    if not isinstance(layer, layers.BatchNormalization):
        layer.trainable = False

# Recompila o modelo com uma taxa de aprendizado baixa
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-5),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Continua o treinamento
initial_epoch = len(history_feature_extraction.epoch)
history_fine_tuning = model.fit(
    train_ds,
    epochs=initial_epoch + 15,
    initial_epoch=initial_epoch,
    validation_data=val_ds,
    callbacks=callbacks
)
```

---

## 7. Avalia√ß√£o do Modelo

Ap√≥s o treinamento, avaliamos a performance final do modelo no conjunto de teste, que n√£o foi utilizado em nenhuma etapa do treino ou ajuste. Isso nos d√° uma medida real da capacidade de generaliza√ß√£o do modelo.

```python
# Avalia no conjunto de teste
test_loss, test_accuracy = model.evaluate(test_ds)
print(f"\nAcur√°cia no teste: {test_accuracy * 100:.2f}%")

# Gera previs√µes para a matriz de confus√£o
y_true = np.concatenate([y for x, y in test_ds], axis=0)
y_pred_probs = model.predict(test_ds)
y_pred = (y_pred_probs > 0.5).astype(int).flatten()

# Plota a matriz de confus√£o e o relat√≥rio de classifica√ß√£o
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

cm = confusion_matrix(y_true, y_pred)
class_names = ds_info.features['label'].names
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Previsto')
plt.ylabel('Real')
plt.title('Matriz de Confus√£o')
plt.show()

print("\nRelat√≥rio de Classifica√ß√£o:")
print(classification_report(y_true, y_pred, target_names=class_names))
```
Com esta abordagem, √© esperado alcan√ßar uma **acur√°cia superior a 98%**.

Para salvar o modelo treinado, use o seguinte comando:
```python
# Salva o modelo
model.save('cats_vs_dogs_classifier.h5')
```
